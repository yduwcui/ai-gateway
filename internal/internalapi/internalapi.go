// Copyright Envoy AI Gateway Authors
// SPDX-License-Identifier: Apache-2.0
// The full text of the Apache license is available in the LICENSE file at
// the root of the repo.

// Package internalapi provides constants and functions used across the boundary
// among controller, extension server and extproc.
package internalapi

import (
	"fmt"
	"strings"

	aigv1a1 "github.com/envoyproxy/ai-gateway/api/v1alpha1"
)

const (
	// InternalEndpointMetadataNamespace is the namespace used for the dynamic metadata for internal use.
	InternalEndpointMetadataNamespace = "aigateway.envoy.io"
	// InternalMetadataBackendNameKey is the key used to store the backend name
	InternalMetadataBackendNameKey = "per_route_rule_backend_name"
)

const (
	// EndpointPickerHeaderKey is the header key used to specify the target backend endpoint.
	// This is the default header name in the reference implementation:
	// https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/2b5b337b45c3289e5f9367b2c19deef021722fcd/pkg/epp/server/runserver.go#L63
	EndpointPickerHeaderKey = "x-gateway-destination-endpoint"
)

const (
	// XDSClusterMetadataKey is the key used to access cluster metadata in xDS attributes
	XDSClusterMetadataKey = "xds.cluster_metadata"
	// XDSUpstreamHostMetadataKey is the key used to access upstream host metadata in xDS attributes
	XDSUpstreamHostMetadataKey = "xds.upstream_host_metadata"
)

// PerRouteRuleRefBackendName generates a unique backend name for a per-route rule,
// i.e., the unique identifier for a backend that is associated with a specific
// route rule in a specific AIGatewayRoute.
func PerRouteRuleRefBackendName(namespace, name, routeName string, routeRuleIndex, refIndex int) string {
	return fmt.Sprintf("%s/%s/route/%s/rule/%d/ref/%d", namespace, name, routeName, routeRuleIndex, refIndex)
}

const (
	// AIGatewayGeneratedHTTPRouteAnnotation is the annotation key used to mark
	// HTTPRoute resources that are generated by the AI Gateway controller.
	AIGatewayGeneratedHTTPRouteAnnotation = "ai-gateway-generated"
)

// ParseRequestHeaderLabelMapping parses comma-separated key-value pairs for header-to-label mapping.
// The input format is "header1:label1,header2:label2" where header names are HTTP request
// headers and label names are Prometheus metric labels.
// Example: "x-team-id:team_id,x-user-id:user_id".
//
// Note: This serves a different purpose than OTEL's OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST,
// which captures headers as span attributes for tracing. This function creates Prometheus metric labels
// from headers with custom naming (e.g., x-team-id â†’ team_id) for proper Prometheus conventions.
func ParseRequestHeaderLabelMapping(s string) (map[string]string, error) {
	if s == "" {
		return nil, nil
	}

	result := make(map[string]string)
	pairs := strings.Split(s, ",")

	for i, pair := range pairs {
		pair = strings.TrimSpace(pair)
		if pair == "" {
			return nil, fmt.Errorf("empty header-label pair at position %d", i+1)
		}

		parts := strings.SplitN(pair, ":", 2)
		if len(parts) != 2 {
			return nil, fmt.Errorf("invalid header-label pair at position %d: %q (expected format: header:label)", i+1, pair)
		}

		header := strings.TrimSpace(parts[0])
		label := strings.TrimSpace(parts[1])

		if header == "" || label == "" {
			return nil, fmt.Errorf("empty header or label at position %d: %q", i+1, pair)
		}

		result[header] = label
	}

	return result, nil
}

// ModelNameHeaderKeyDefault is the default header key for the model name.
const ModelNameHeaderKeyDefault = aigv1a1.AIModelHeaderKey

// ModelNameHeaderKey is the configurable header key whose value is set by the gateway
// based on the model extracted from the request body.
//
// This header is automatically populated by the gateway and cannot be set by end users
// as it will be overwritten. The flow is:
//  1. Router filter extracts OriginalModel from request body and sets this header
//  2. HTTPRoute uses this header value for model-based routing
//  3. If backend has ModelNameOverride, the header is updated with the override value
//  4. Metrics and observability systems use the final header value
//
// Defaults to ModelNameHeaderKeyDefault.
type ModelNameHeaderKey = string

// ModelNameOverride represents a backend-specific model name that overrides
// the OriginalModel in the client request to the router.
//
// Configuration:
//   - Set via aigv1a1.AIGatewayRouteRuleBackendRef
//   - Replaces the OriginalModel with a backend-specific model name
//
// Example:
//   - server requests: "llama3-2-1b"
//   - Override to: "us.meta.llama3-2-1b-instruct-v1:0" (for AWS Bedrock)
//
// Effects:
//   - Updates the header specified by ModelNameHeaderKey
//   - Used by routing, rate limiting, and observability systems
type ModelNameOverride = string

// OriginalModel is the model name extracted from the incoming request body
// before any virtualization applies.
//
// Flow:
//  1. Router filter extracts model from request body
//  2. If ModelNameOverride is configured, RequestModel differs from OriginalModel
//  3. Provider responds with ResponseModel (may differ from RequestModel)
//
// Example:
//  1. OriginalModel: OpenAI Client sends: {"model": "gpt-5"}
//  2. RequestModel: ModelNameOverride replaces with "gpt-5-nano"
//  3. ResponseModel: OpenAI Platform sends: {"model": "gpt-5-nano-2025-08-07"}
//
// ### OpenTelemetry
//
// In OpenTelemetry Generative AI Metrics, this is an attribute on metrics such
// as "gen_ai.server.token.usage". For example, an OpenAI Chat Completion
// request to the "gpt-5" model results in a plain text string attribute:
// "gen_ai.original.model" -> "gpt-5"
type OriginalModel = string

// RequestModel is the name of the model sent in the request to perform a
// completion or to create embeddings.
//
// This is either the model received by the router's OpenAI Chat Completions or
// Embeddings endpoints, or a ModelNameOverride.
//
// This is not necessarily the same as ResponseModel, and in some cases like
// Azure OpenAI Service, this field isn't read at all.
//
// ### OpenTelemetry
//
// The RequestModel is a key attribute for correlating metrics with spans.
//
// In OpenInference (span semantics), this is the "model" field of invocation
// parameters, explaining how the LLM was invoked. For example, an OpenAI
// Chat Completion request to the "gpt-5-nano" model results in an JSON string
// attribute: "llm.invocation_parameters" -> {"model": "gpt-5-nano"}
//
// In OpenTelemetry Generative AI Metrics, this is an attribute on metrics such
// as "gen_ai.server.token.usage". For example, an OpenAI Chat Completion
// request to the "gpt-5-nano" model results in a plain text string attribute:
// "gen_ai.request.model" -> "gpt-5-nano"
type RequestModel = string

// ResponseModel is the name of the model that generated a response to a
// completion or embeddings request.
//
// ### Relationship to RequestModel
//
// This may differ from the RequestModel unless the provider is deterministic:
//   - Static Model Execution (AWS Bedrock)
//   - Deterministic Snapshot Mapping (GCP providers)
//
// In virtualized providers, this may be different:
//   - URI-Based Resolution (Azure OpenAI)
//   - Automatic Routing & Resolution: (OpenAI Platform)
//
// See https://aigateway.envoyproxy.io/docs/capabilities/traffic/model-name-virtualization
//
// ### OpenTelemetry
//
// The ResponseModel is even more important that RequestModel for evaluation
// use cases as it is the only field that authoritatively explains the model
// used for a completion. It is a key attribute for correlating metrics with
// spans.
//
// In OpenInference (span semantics), this is the "model_name" attribute.
// parameters, explaining how the LLM was invoked. For example, an OpenAI
// Chat Completion request to the "gpt-5-nano" model results in a plain text
// attribute of the latest model: "llm.model_name" -> "gpt-5-nano-2025-08-07"
//
// In OpenTelemetry Generative AI Metrics, this is an attribute on metrics such
// as "gen_ai.server.token.usage". For example, an OpenAI Chat Completion
// request to the "gpt-5-nano" model results in a plain text attribute of the
// latest model: "gen_ai.response.model" -> "gpt-5-nano-2025-08-07"
type ResponseModel = string
