# Copyright Envoy AI Gateway Authors
# SPDX-License-Identifier: Apache-2.0
# The full text of the Apache license is available in the LICENSE file at
# the root of the repo.

apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: inference-extension-example
spec:
  controllerName: gateway.envoyproxy.io/gatewayclass-controller
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: inference-extension-example
  namespace: default
spec:
  gatewayClassName: inference-extension-example
  listeners:
    - name: http
      protocol: HTTP
      port: 80
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIGatewayRoute
metadata:
  name: inference-extension-example
  namespace: default
spec:
  schema:
    name: OpenAI
  targetRefs:
    - name: inference-extension-example
      kind: Gateway
      group: gateway.networking.k8s.io
  rules:
    - matches:
        - headers:
            - type: Exact
              name: x-target-inference-extension
              value: "yes"
      backendRefs:
        - name: inference-extension-example-pool  # The name of the InferencePool that this route will route to.
          # Explicitly specify the kind of the backend to be InferencePool.
          kind: InferencePool
---
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: inference-extension-example-pool
spec:
  targetPortNumber: 8080
  selector:
    # Select multiple AIServiceBackend objects to bind to the InferencePool.
    app: my-backend
  extensionRef:
    # Specify the static name "envoy-ai-gateway" to bind the InferencePool to the Envoy AI Gateway.
    # This indicates that the InferencePool will be managed by the Envoy AI Gateway.
    name: envoy-ai-gateway
---
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferenceModel
metadata:
  name: inference-extension-example
spec:
  modelName: mistral:latest
  criticality: Critical
  poolRef:
    # Bind the InferenceModel to the InferencePool.
    name: inference-extension-example-pool
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
  name: inference-extension-example-testupstream
  namespace: default
  labels:
    # Indicate the backend is selected by the InferencePool.
    app: my-backend
spec:
  schema:
    name: OpenAI
  backendRef:
    name: inference-extension-example-testupstream
    kind: Service
    port: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: inference-extension-example-testupstream
  namespace: default
spec:
  selector:
    app: inference-extension-example-testupstream
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  # The headless service allows the IP addresses of the pods to be resolved via the Service DNS.
  clusterIP: None
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-extension-example-testupstream
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: inference-extension-example-testupstream
  template:
    metadata:
      labels:
        app: inference-extension-example-testupstream
    spec:
      containers:
        - name: testupstream
          image: docker.io/envoyproxy/ai-gateway-testupstream:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
          env:
            - name: TESTUPSTREAM_ID
              value: test
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 1
            periodSeconds: 1
