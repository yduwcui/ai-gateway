# Copyright Envoy AI Gateway Authors
# SPDX-License-Identifier: Apache-2.0
# The full text of the Apache license is available in the LICENSE file at
# the root of the repo.

volumes:
  aigw-target:  # Stores the binary between aigw-build and aigw
  envoy-cache:  # Stores downloaded Envoy versions

services:
  # aigw-build builds the Envoy AI Gateway CLI binary, so you can use main code.
  aigw-build:
    image: golang:1.25
    container_name: aigw-build
    working_dir: /workspace
    volumes:
      - ../..:/workspace
      - aigw-target:/workspace/out
    command: ["bash", "-c", "go build -buildvcs=false -o /workspace/out/aigw ./cmd/aigw"]

  # phoenix is an OpenTelemetry compatible LLM eval system which accepts trace
  # spans and has special handling for OpenInference LLM semantic conventions.
  # See https://arize.com/docs/phoenix
  phoenix:
    image: arizephoenix/phoenix:latest
    container_name: phoenix
    ports:
      - "6006:6006"
    environment:
      PHOENIX_ENABLE_AUTH: "false"

  # ollama-pull pulls the models defined in .env.ollama, to avoid 404s.
  ollama-pull:
    image: alpine/ollama
    container_name: ollama-pull
    environment:
      OLLAMA_HOST: localhost:11434  # intentionally not 127.0.0.1
    env_file:
      - ../../.env.ollama
    entrypoint: sh
    command: -c 'env | grep _MODEL | cut -d= -f2 | xargs -I{} ollama pull {}'
    extra_hosts:  # send localhost traffic to the docker host, e.g. your laptop
      - "localhost:host-gateway"

  # aigw is the Envoy AI Gateway CLI a.k.a standlane mode.
  aigw:
    image: debian:trixie-slim
    container_name: aigw
    depends_on:
      aigw-build:
        condition: service_completed_successfully
      phoenix:
        condition: service_started
      ollama-pull:
        condition: service_completed_successfully
    environment:
      # Below sets the OTLP collector to Phoenix, using a shorter span delay.
      # See https://opentelemetry.io/docs/specs/otel/configuration/sdk-environment-variables/
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://phoenix:6006
      - OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
      - OTEL_BSP_SCHEDULE_DELAY=100
      # Below are default values for span redaction in OpenInference.
      # See https://github.com/Arize-ai/openinference/blob/main/spec/configuration.md
      - OPENINFERENCE_HIDE_INPUTS=false
      - OPENINFERENCE_HIDE_OUTPUTS=false
      - OPENAI_HOST=host.docker.internal
    ports:
      - "1975:1975"  # OpenAI compatible endpoint at /v1
      - "1064:1064"  # Prometheus endpoint at /metrics
    extra_hosts:  # localhost:host-gateway trick doesn't work with aigw
      - "host.docker.internal:host-gateway"
    volumes:
      - aigw-target:/app
      - ./ai-gateway-local.yaml:/config.yaml:ro
      - envoy-cache:/tmp/envoy-gateway
    working_dir: /app
    command: ["sh", "-c", "apt-get update && apt-get install -y ca-certificates && ./aigw run /config.yaml"]

  # chat-completion is the standard OpenAI client (`openai` in pip), instrumented
  # with the following OpenTelemetry instrumentation libraries:
  # - openinference-instrumentation-python (LLM chat completion spans)
  # - opentelemetry-instrumentation-httpx (HTTP client spans and trace headers)
  chat-completion:
    build:
      context: ../../tests/internal/testopeninference
      dockerfile: Dockerfile.openai_client
    container_name: chat-completion
    profiles: ["test"]
    env_file:
      - ../../.env.ollama
    environment:
      - OPENAI_BASE_URL=http://aigw:1975/v1
      - OPENAI_API_KEY=unused
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://phoenix:6006
      - OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
      - OTEL_BSP_SCHEDULE_DELAY=100
    depends_on:
      aigw:
        condition: service_started
