# Copyright Envoy AI Gateway Authors
# SPDX-License-Identifier: Apache-2.0
# The full text of the Apache license is available in the LICENSE file at
# the root of the repo.

configs:
  # MCP servers configuration for aigw
  mcp-config:
    content: |
      {
        "mcpServers": {
          "kiwi": {
            "type": "http",
            "url": "https://mcp.kiwi.com"
          }
        }
      }

services:
  # otel-tui is a terminal UI for viewing OpenTelemetry traces and metrics
  otel-tui:
    image: ymtdzzz/otel-tui:latest
    container_name: otel-tui
    profiles: ["otel-tui"]
    ports:
      - "4318:4318"
    stdin_open: true
    tty: true

  # phoenix is an OpenTelemetry compatible LLM eval system which accepts trace
  # spans and has special handling for OpenInference LLM semantic conventions.
  # See https://arize.com/docs/phoenix
  phoenix:
    image: arizephoenix/phoenix:latest
    container_name: phoenix
    profiles: ["phoenix"] # Only start when explicitly requested
    ports:
      - "6006:6006"
    environment:
      PHOENIX_ENABLE_AUTH: "false"

  # ollama-pull pulls the models defined in .env.ollama, to avoid 404s.
  ollama-pull:
    image: alpine/ollama
    container_name: ollama-pull
    environment:
      OLLAMA_HOST: localhost:11434 # intentionally not 127.0.0.1
    env_file:
      - ../../.env.ollama
    entrypoint: sh
    command: -c 'env | grep _MODEL | cut -d= -f2 | xargs -I{} ollama pull {}'
    extra_hosts: # send localhost traffic to the docker host, e.g. your laptop
      - "localhost:host-gateway"

  # aigw is the Envoy AI Gateway CLI a.k.a standalone mode.
  # Uses the official image by default, or builds from source with --build flag.
  aigw:
    image: envoyproxy/ai-gateway-cli:latest
    # Note: Run `make build.aigw GOOS_LIST=linux` from the project root prior
    # to `docker compose -f docker-compose-otel.yaml up --build --wait -d` here.
    build:
      context: ../..
      dockerfile: Dockerfile
      args:
        VARIANT: base-nossl
        COMMAND_NAME: aigw
    container_name: aigw
    depends_on:
      ollama-pull:
        condition: service_completed_successfully
    env_file:
      - .env.otel.${COMPOSE_PROFILES:-console}
    environment:
      - OPENAI_BASE_URL=http://host.docker.internal:11434/v1
      - OPENAI_API_KEY=unused
      # Map HTTP request headers to otel span and metric attributes for session tracking
      # Format: header:attribute,header:attribute
      - OTEL_AIGW_SPAN_REQUEST_HEADER_ATTRIBUTES=x-session-id:session.id,x-user-id:user.id
      - OTEL_AIGW_METRICS_REQUEST_HEADER_ATTRIBUTES=x-user-id:user.id
    configs:
      - source: mcp-config
        target: /etc/aigw/mcp-servers.json
    ports:
      - "1975:1975" # OpenAI compatible endpoint at /v1, MCP server at /mcp
      - "1064:1064" # Admin server: /metrics (Prometheus) and /health endpoints
    extra_hosts: # localhost:host-gateway trick doesn't work with aigw
      - "host.docker.internal:host-gateway"
    command: ["run", "--mcp-config", "/etc/aigw/mcp-servers.json"]

  # chat-completion is the standard OpenAI client (`openai` in pip), instrumented
  # with the following OpenTelemetry instrumentation libraries:
  # - openinference-instrumentation-python (LLM chat completion spans)
  # - opentelemetry-instrumentation-httpx (HTTP client spans and trace headers)
  chat-completion:
    build:
      context: ../../tests/internal/testopeninference
      dockerfile: Dockerfile.openai_client
      target: chat-completion
    container_name: chat-completion
    profiles: ["test"]
    env_file:
      - ../../.env.ollama
      - .env.otel.${COMPOSE_PROFILES:-console}
    environment:
      - OPENAI_BASE_URL=http://aigw:1975/v1
      - OPENAI_API_KEY=unused

  # create-embeddings is the standard OpenAI client (`openai` in pip), instrumented
  # with the following OpenTelemetry instrumentation libraries:
  # - openinference-instrumentation-openai (embeddings spans)
  # - opentelemetry-instrumentation-httpx (HTTP client spans and trace headers)
  create-embeddings:
    build:
      context: ../../tests/internal/testopeninference
      dockerfile: Dockerfile.openai_client
      target: create-embeddings
    container_name: create-embeddings
    profiles: ["test"]
    env_file:
      - ../../.env.ollama
      - .env.otel.${COMPOSE_PROFILES:-console}
    environment:
      - OPENAI_BASE_URL=http://aigw:1975/v1
      - OPENAI_API_KEY=unused

  # completion is the standard OpenAI client (`openai` in pip), instrumented
  # with the following OpenTelemetry instrumentation libraries:
  # - openinference-instrumentation-openai (completions spans)
  # - opentelemetry-instrumentation-httpx (HTTP client spans and trace headers)
  completion:
    build:
      context: ../../tests/internal/testopeninference
      dockerfile: Dockerfile.openai_client
      target: completion
    container_name: completion
    profiles: ["test"]
    env_file:
      - ../../.env.ollama
      - .env.otel.${COMPOSE_PROFILES:-console}
    environment:
      - OPENAI_BASE_URL=http://aigw:1975/v1
      - OPENAI_API_KEY=unused

  # mcp is a test client for calling MCP tools through aigw.
  # TODO: add client tracing + mcp propagation to this
  mcp:
    image: ghcr.io/modelcontextprotocol/inspector:latest
    container_name: mcp
    profiles: ["test"]
    entrypoint: node
    command:
      - cli/build/cli.js
      - --cli
      - http://aigw:1975/mcp
      - --method
      - tools/call
      - --tool-name
      - kiwi__search-flight
      - --tool-arg
      - flyFrom=NYC
      - --tool-arg
      - flyTo=LAX
      - --tool-arg
      - departureDate=15/12/2025
